
# Concourse-CI Experience & Feedback

Written: 2020-01-17 

## Intro 
This is a list of the rough points I found when setting up a CI/CD pipeline in [Concourse CI](http://concourse-ci.org) over the last month. 

While there are certainly some great ideas, overall I feel like the product is just not mature enough. 

----


Pipeline control
---- 

Take an example very simplified application pipeline

    1. Build Code
        1.1 Clone Repo
        1.2 Create Assets 
    2. Run tests
        2.1 Unit Tests 
            2.1.1 Suite 1
            2.1.2 Suite 2
            2.1.3 Suite ... 
            2.3.4 Suite N 
        2.2 Integration tests
            2.2.1 Suite 1
            2.2.2 Suite 2
            2.2.3 Suite ...
            2.2.N Suite N 
    3. Publish / Deploy


With concourse, these steps are called jobs, and each job consists of many tasks.  

For instance, Running a unit test might require that you run one task to pull down and extract the versioned asset, another to run the unit tests, and a third to push any test artifacts up to a remote location. 

For some applications, running tests might be a long and (computationally) expensive task. Running suites of tests in parallel reduces the overall time taken, and for integration tests where it's more likely you'll encounter a transient error, breaking it up allows you to re-run just the failed portion.

What you generally don't want, unless you have unlimited money and compute resources, is many many builds runing at once. 
You generally want to stop Step 1's job from kicking off, until the rest of the pipeline steps from the previous pipeline run have completed. 

Concourse basically lets you pick any two from this list: 

a. Don't queue multiple pipeline builds.  

b. Parallelise some portions within the pipeline.   

c. Able to retry specific portions of the pipeline manually


a+b = use the job's serial_groups feature, and use the parallel task feature. This takes away your ability to retry one specific failed task manually. 

a+c = use the job's serial_groups feature, and have paralellisable steps as their own pipeline jobs. This takes away your ability to parallelise tasks. 

b+c = Don't use serial_groups, have each step as a pipeline job. This means you'll have excessively large numbers of un-needed jobs queueing. 

As far as I can tell, there's no plan to fix this. 


No Build Queue / Random Execution Order
---

A run of a job is called a build, and it's for that specific job. 
While you can stop multiple builds for the same job from being executed at once - by the use of `serial: true` - the builds themselves are not queued at all. 

Lets consider two builds for two different jobs - Build A and Build B. Build A is started first, and then a minute later Build B is started. 
Each job has many steps, each of which take a long time to execute. 

For this environment, we have one worker and it's configured to only run one job at a time. 

You might reasonably expect that since Build A is started first, that Build A would have all of it's tasks executed to completion, followed by Build B. 

However any task that's ready to be executed (i.e it's the next in line for any running job) can be executed in any order.

This might mean that you have an execution that looks like so: 

    - Build A, Task 1
    - Build B, Task 1
    - Build B, Task 2 
    - Build B, Task 3
    - Build A, Task 1

This cannot be controlled for. 

While there is a fix coming, it's been coming for a long while. 

On a related note, it's possible to get multiple builds queued for the same job. I honestly don't know how it's happening, I suspect because there's multiple triggers and so-forth. It's a complicated queue. 


No good way to control task scheduling onto workers
--- 

Take a scenario where I have tasks that have specific requirements - for instance, they might require a GPU, or they might be particularly memory heavy. 

While I can control that jobs/tasks are only run on specific tagged nodes - I can't control how many of those jobs are scheduled onto those workers at once. 
This becomes a problem when you have mixed weights of tasks - for instance running a git clone and then tarring the contents is pretty light. Something that eats 32 CPU cores and 64GB RAM for 15 minutes isn't. 

I can control the number of tasks executed per worker. But only on a global basis. This means that if I have one instance with 4CPU/16GB RAM and another with 8CPU/32GB RAM I can't schedule double the number of same-weight tasks onto the larger instance. 

The effect of this is that I have to limit the number tasks per worker to either be a single task per worker (and have worker instances sized to meet the requirements of the largest task) - or have instances sized for some multiple of the largest worker.  

In either case, I end up wasting resources. 




No good support for scaling workers up/down
--- 

This comprises multiple parts. 

First, there's a worker SSH key. 
This isn't used for identity except for the case of team workers. 
There's also no way to add keys to the web nodes at runtime AND have them recognise the new key.  This means if you need to add worker keys at runtime, you need to recycle ALL of your web nodes. The approach to this is just to have every worker have the same shared key. [There's more to it](https://discuss.concourse-ci.org/t/creating-workers-on-demand/1994/6)

Second, because there's no job queue, I can't extract useful information about how many jobs/tasks are waiting for a worker.  This means I have to guess how to scale. 

Third, because there's no job queue - tasks that were scheduled on an instance that has now gone away end up errored. While I could, in theory, run a land/drain command - this isn't always possible within scaling times given by, say, AWS. 

Fourth, because there's no option to have Concourse spin up/shut down workers based on what it needs resource-wise, using auto-scaling will inevitably result in in-progress jobs being cancelled if they can't finish before the autoscaling rule kills the instance. 


No good way to override/ignore errors
--- 

Sometimes you just need to push something out, even if the tests say it's failed. It's not great, but you can make a judgement call that you can accept some broken unit tests in return for just getting a release out. 

As far as I can tell there's no good way to solve this, except having a parallel version of your build queue. 


No support for test artifacts, in particular test results. 
--- 

Simple, honestly - it needs some way of results for a task/job, other than just logs. 

Ideally, also some kind of ability for it to link to other artifacts, even if they're stored elsewhere. 


No good support for identifying changes between the last successful build and the current one. 
--- 

It's currently not easy to figure out the git change log between the last sucessful build of the pipeline and this current run. 

The only solution I can think of is to pass along your git repo resource all the way to the last step, and then store an artifact with that git hash, then read the latest version of that artifact in your build step. 

Bit of a pain in the arse. 


No good support for versioning
--- 

While there's semver, it's missing a whole bunch of functionality. 

Things that it's either difficult/not possible: 

 - Have date-based version numbers. eg: 2020.01.14.(day's build number)
 - Have embed git short hash in the version. eg. 2020.01.14.0-abc1234
 - When a pipeline completes, tag that git commit with the version number. 


 Pipeline visualisation
 --- 

 While it's definitely a neat idea to be able to see how your pipeline sticks together and passes data along, for anything moderately complex it's actually a pain. 

 More than a few builds happening has so much animation that there's a huge amount of CPU usage. 

 Any moderately complex pipeline quickly becomes unreadable with lines going everywhere. There is the option to group parts of your pipeline, but then this removes the context of how your pipeline fits together.   

 On a similar note - the visualisation requires using `passed: \[previous-job\]` on a resource that was used/published by the previous job. This means if you DO want the visualsation, you now complicate jobs with pulling get on resources it doesn't care about, and makes the build slower.

Container management
--- 

For something that's so heavily dependant upon containers, the level of control for containers is actually pretty terrible. 
You can't pass along limits/controls to the containers - eg CPU/Memory limits. 

There's a dependency on runc. This breaks your ability to use it within some environments - eg running the worker as a container on Amazon ECS, which uses Amazon Linux by default.  This dependency is going away, apparently, but it's still there. 

Resources are always checked
---

If you want to have a pipeline triggered by a webhook, there's a way to do that - have a resource, and set the webhook property on it. 

However if you ONLY want it triggered by a webhook, sorry - there's no way to turn off the scheduled checking, you just have to put it up to a high number. [This has been an open issue for more than 3.5 years](https://github.com/concourse/concourse/issues/575), with no signs of it being implemented. 


Pipeline Management
--- 

Managing a reasonably complicated pipeline becomes a major pain, given it combines not only the pipeline steps, but the configuration for all of them. 
So you end up with a pipeline YAML file that's nearly impossible to navigate.  

While you can define tasks as external files (and this is necessary if you want to template out tasks), this means giving your pipeline access to the git repo that hosts your tasks. If you do this, it makes testing changes to your tasks locally more difficult. 

Support for includes would be very helpful here. A workaround that I've started adopting is to build the pipeline as a Jinja2 templated YAML file, and then run a jinja2 transform on it before upload. 


Pipeline Variables 
--- 

These are a bit funky and kinda painful - if you want to emit a variable from one task that is then used as a parameter for another task (say generating a message that should be sent out via Slack) - then the resource you're using (the Slack resource) has to support using files. 

Annoyingly, you can't pass these variables from one job to another, you have to put them as artifacts and store them externally. 



Nothing built in
--- 

While the idea that nothing is built in, and therefore can be extended the way you want it is a good idea in theory, I'm using a CI/CD system because I didn't want to build one myself.
At some point the missing features just add up into making this unsuitable for my needs. 

There's also, oddly enough, a bunch of missing extension points which make implementing a bunch of the things I've mentioned above difficult. 


Small community / Issue management
--- 

Judging this is hard, but it certainly seems to not be a hugely popular CI/CD system.  
This leads into a lot of the issues and discussions seeming like ghost towns with issues being open for years. 
Also, still active issues are regularly closed because of the use of an idle bot, which helps clean up the 'open' list somewhat. 

